{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vietnamese_Language_Model.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "5QbrZFvdGJZo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "d84d9684-6868-45d1-fb17-b3109e7a6556"
      },
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bbRGOLPtHZwN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# def load_sequences(part):\n",
        "#     with open('drive/My Drive/Machine_Learning-prj/vietnamese_language_model/data/sequences'+ str(part) + '.0', 'rb') as f:\n",
        "#         sequences = pickle.load(f)\n",
        "#     return sequences[:1000000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wpvMVNCZHopX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# sequences = load_sequences(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v-KEh80wHv8Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# len(sequences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dsJMBYU2HxtB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import keras\n",
        "\n",
        "# tokenizer = keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\]^`{|}~ ')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NvuZPG5dIKrG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# tokenizer.fit_on_texts(sequences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ltmnbmD5ZHOf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# with open('drive/My Drive/Machine_Learning-prj/vietnamese_language_model/tokenizer_S1.pkl', 'wb') as f:\n",
        "#     pickle.dump(tokenizer, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bR0UYclHQpPO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# sequences_digit = tokenizer.texts_to_sequences(sequences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "26ff3gLIZT8R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# with open('drive/My Drive/Machine_Learning-prj/vietnamese_language_model/sequences_digit_S1.pkl', 'wb') as f:\n",
        "#     pickle.dump(sequences_digit, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "unpJQ6ZWT3eX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#del sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cCQenKUvgp8M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "e806fa1f-b1b2-48e6-e238-8dddfa57d2e7"
      },
      "cell_type": "code",
      "source": [
        " with open('drive/My Drive/Machine_Learning-prj/vietnamese_language_model/tokenizer_S1.pkl', 'rb') as f:\n",
        "    tokenizer = pickle.load(f)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "pgowNLY6g7to",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('drive/My Drive/Machine_Learning-prj/vietnamese_language_model/sequences_digit_S1.pkl', 'rb') as f:\n",
        "    sequences_digit = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KoRLb80DQfcX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iNOFV1pFbD9M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def data_generator(sequences_digit, batch_size):\n",
        "    while True:\n",
        "        batch_paths = np.random.choice(a = len(sequences_digit), size = batch_size)\n",
        "        input = []\n",
        "        output = []\n",
        "        \n",
        "        for i in batch_paths:\n",
        "            input.append(sequences_digit[i][:-1])\n",
        "            output.append(sequences_digit[i][-1])\n",
        "        input = np.array(input)\n",
        "        output = keras.utils.to_categorical(output, num_classes=vocab_size)\n",
        "        output = np.array(output)\n",
        "        \n",
        "        yield (input, output)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eqLgryZscNOr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation, GRU, Dense, Input, Add, Concatenate, Reshape, Lambda, BatchNormalization, Dropout, Embedding, LSTM\n",
        "from keras.optimizers import SGD, RMSprop\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wBdeXfaWS7aS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "6a064449-cd23-4f0f-f74c-aecaf6dc3ab6"
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length=50))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LSTM(512, return_sequences=True))\n",
        "model.add(LSTM(512))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 50, 50)            1733100   \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 50, 50)            200       \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 50, 512)           1153024   \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 512)               2099200   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 100)               51300     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 34662)             3500862   \n",
            "=================================================================\n",
            "Total params: 8,538,086\n",
            "Trainable params: 8,537,786\n",
            "Non-trainable params: 300\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q_v-eg30cMZr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZVgFPN4MSjYp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "a2b6d1ef-159c-433f-9d2c-0edf3d4a11c8"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "print(\"Loading pre-trained weight\")\n",
        "model.load_weights('drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading pre-trained weight\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GXeW3lr4RcKs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "filepath = 'drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5'\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dZHW5_UUdGs4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size=512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mIcqO6UWcsT_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4872
        },
        "outputId": "2409728b-5ace-4ed8-978d-22cace15c014"
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(generator=data_generator(sequences_digit, batch_size), steps_per_epoch=(len(sequences_digit)//batch_size) , epochs=100, verbose=1, callbacks=callbacks_list)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1953/1953 [==============================] - 744s 381ms/step - loss: 3.2499 - acc: 0.3510\n",
            "\n",
            "Epoch 00001: acc improved from -inf to 0.35099, saving model to drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5\n",
            "Epoch 2/100\n",
            "1953/1953 [==============================] - 739s 379ms/step - loss: 3.2380 - acc: 0.3523\n",
            "\n",
            "Epoch 00002: acc improved from 0.35099 to 0.35228, saving model to drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5\n",
            "Epoch 3/100\n",
            "1953/1953 [==============================] - 739s 378ms/step - loss: 3.2206 - acc: 0.3551\n",
            "\n",
            "Epoch 00003: acc improved from 0.35228 to 0.35511, saving model to drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5\n",
            "Epoch 4/100\n",
            "1953/1953 [==============================] - 740s 379ms/step - loss: 3.2057 - acc: 0.3569\n",
            "\n",
            "Epoch 00004: acc improved from 0.35511 to 0.35693, saving model to drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5\n",
            "Epoch 5/100\n",
            "1953/1953 [==============================] - 741s 379ms/step - loss: 3.1943 - acc: 0.3587\n",
            "\n",
            "Epoch 00005: acc improved from 0.35693 to 0.35871, saving model to drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5\n",
            "Epoch 6/100\n",
            "1953/1953 [==============================] - 741s 379ms/step - loss: 3.1766 - acc: 0.3612\n",
            "\n",
            "Epoch 00006: acc improved from 0.35871 to 0.36115, saving model to drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5\n",
            "Epoch 7/100\n",
            "1953/1953 [==============================] - 741s 379ms/step - loss: 3.1686 - acc: 0.3621\n",
            "\n",
            "Epoch 00007: acc improved from 0.36115 to 0.36209, saving model to drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5\n",
            "Epoch 8/100\n",
            "1953/1953 [==============================] - 741s 379ms/step - loss: 3.1553 - acc: 0.3634\n",
            "\n",
            "Epoch 00008: acc improved from 0.36209 to 0.36341, saving model to drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5\n",
            "Epoch 9/100\n",
            "1953/1953 [==============================] - 740s 379ms/step - loss: 3.1501 - acc: 0.3645\n",
            "\n",
            "Epoch 00009: acc improved from 0.36341 to 0.36449, saving model to drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5\n",
            "Epoch 10/100\n",
            "1953/1953 [==============================] - 740s 379ms/step - loss: 3.1377 - acc: 0.3660\n",
            "\n",
            "Epoch 00010: acc improved from 0.36449 to 0.36600, saving model to drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5\n",
            "Epoch 11/100\n",
            "1953/1953 [==============================] - 741s 379ms/step - loss: 3.1312 - acc: 0.3669\n",
            "\n",
            "Epoch 00011: acc improved from 0.36600 to 0.36686, saving model to drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5\n",
            "Epoch 12/100\n",
            "1953/1953 [==============================] - 737s 378ms/step - loss: 3.1213 - acc: 0.3682\n",
            "\n",
            "Epoch 00012: acc improved from 0.36686 to 0.36817, saving model to drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5\n",
            "Epoch 13/100\n",
            "1953/1953 [==============================] - 735s 376ms/step - loss: 3.1146 - acc: 0.3693\n",
            "\n",
            "Epoch 00013: acc improved from 0.36817 to 0.36925, saving model to drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5\n",
            "Epoch 14/100\n",
            "1953/1953 [==============================] - 732s 375ms/step - loss: 3.1045 - acc: 0.3700\n",
            "\n",
            "Epoch 00014: acc improved from 0.36925 to 0.37003, saving model to drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5\n",
            "Epoch 15/100\n",
            "1953/1953 [==============================] - 726s 372ms/step - loss: 3.1064 - acc: 0.3699\n",
            "\n",
            "Epoch 00015: acc did not improve from 0.37003\n",
            "Epoch 16/100\n",
            "1953/1953 [==============================] - 728s 373ms/step - loss: 3.0992 - acc: 0.3711\n",
            "\n",
            "Epoch 00016: acc improved from 0.37003 to 0.37111, saving model to drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5\n",
            "Epoch 17/100\n",
            "1953/1953 [==============================] - 731s 374ms/step - loss: 3.0843 - acc: 0.3730\n",
            "\n",
            "Epoch 00017: acc improved from 0.37111 to 0.37296, saving model to drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5\n",
            "Epoch 18/100\n",
            "1953/1953 [==============================] - 719s 368ms/step - loss: 3.0745 - acc: 0.3739\n",
            "\n",
            "Epoch 00018: acc improved from 0.37296 to 0.37393, saving model to drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5\n",
            "Epoch 19/100\n",
            "1953/1953 [==============================] - 726s 372ms/step - loss: 3.0690 - acc: 0.3744\n",
            "\n",
            "Epoch 00019: acc improved from 0.37393 to 0.37443, saving model to drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5\n",
            "Epoch 20/100\n",
            "1953/1953 [==============================] - 730s 374ms/step - loss: 3.0586 - acc: 0.3762\n",
            "\n",
            "Epoch 00020: acc improved from 0.37443 to 0.37622, saving model to drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5\n",
            "Epoch 21/100\n",
            "1953/1953 [==============================] - 737s 377ms/step - loss: 3.0532 - acc: 0.3765\n",
            "\n",
            "Epoch 00021: acc improved from 0.37622 to 0.37650, saving model to drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5\n",
            "Epoch 22/100\n",
            "1953/1953 [==============================] - 739s 379ms/step - loss: 3.0540 - acc: 0.3758\n",
            "\n",
            "Epoch 00022: acc did not improve from 0.37650\n",
            "Epoch 23/100\n",
            "1953/1953 [==============================] - 733s 375ms/step - loss: 3.0401 - acc: 0.3779\n",
            "\n",
            "Epoch 00023: acc improved from 0.37650 to 0.37786, saving model to drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5\n",
            "Epoch 24/100\n",
            "1953/1953 [==============================] - 725s 371ms/step - loss: 3.0345 - acc: 0.3788\n",
            "\n",
            "Epoch 00024: acc improved from 0.37786 to 0.37879, saving model to drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5\n",
            "Epoch 25/100\n",
            "1953/1953 [==============================] - 722s 370ms/step - loss: 3.0277 - acc: 0.3794\n",
            "\n",
            "Epoch 00025: acc improved from 0.37879 to 0.37935, saving model to drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5\n",
            "Epoch 26/100\n",
            "1953/1953 [==============================] - 721s 369ms/step - loss: 3.0237 - acc: 0.3804\n",
            "\n",
            "Epoch 00026: acc improved from 0.37935 to 0.38038, saving model to drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5\n",
            "Epoch 27/100\n",
            "1953/1953 [==============================] - 722s 370ms/step - loss: 3.0107 - acc: 0.3824\n",
            "\n",
            "Epoch 00027: acc improved from 0.38038 to 0.38243, saving model to drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5\n",
            "Epoch 28/100\n",
            "1953/1953 [==============================] - 722s 370ms/step - loss: 3.0104 - acc: 0.3819\n",
            "\n",
            "Epoch 00028: acc did not improve from 0.38243\n",
            "Epoch 29/100\n",
            "1953/1953 [==============================] - 717s 367ms/step - loss: 3.0060 - acc: 0.3827\n",
            "\n",
            "Epoch 00029: acc improved from 0.38243 to 0.38273, saving model to drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5\n",
            "Epoch 30/100\n",
            "1953/1953 [==============================] - 715s 366ms/step - loss: 2.9975 - acc: 0.3838\n",
            "\n",
            "Epoch 00030: acc improved from 0.38273 to 0.38377, saving model to drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5\n",
            "Epoch 31/100\n",
            "1953/1953 [==============================] - 716s 367ms/step - loss: 2.9865 - acc: 0.3853\n",
            "\n",
            "Epoch 00031: acc improved from 0.38377 to 0.38532, saving model to drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5\n",
            "Epoch 32/100\n",
            "1953/1953 [==============================] - 717s 367ms/step - loss: 2.9808 - acc: 0.3862\n",
            "\n",
            "Epoch 00032: acc improved from 0.38532 to 0.38623, saving model to drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5\n",
            "Epoch 33/100\n",
            "1953/1953 [==============================] - 717s 367ms/step - loss: 2.9867 - acc: 0.3851\n",
            "\n",
            "Epoch 00033: acc did not improve from 0.38623\n",
            "Epoch 34/100\n",
            "1953/1953 [==============================] - 716s 367ms/step - loss: 2.9700 - acc: 0.3878\n",
            "\n",
            "Epoch 00034: acc improved from 0.38623 to 0.38779, saving model to drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5\n",
            "Epoch 35/100\n",
            "1953/1953 [==============================] - 716s 366ms/step - loss: 2.9657 - acc: 0.3874\n",
            "\n",
            "Epoch 00035: acc did not improve from 0.38779\n",
            "Epoch 36/100\n",
            "1953/1953 [==============================] - 720s 369ms/step - loss: 2.9583 - acc: 0.3881\n",
            "\n",
            "Epoch 00036: acc improved from 0.38779 to 0.38814, saving model to drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5\n",
            "Epoch 37/100\n",
            "1953/1953 [==============================] - 741s 379ms/step - loss: 2.9569 - acc: 0.3889\n",
            "\n",
            "Epoch 00037: acc improved from 0.38814 to 0.38893, saving model to drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5\n",
            "Epoch 38/100\n",
            "1953/1953 [==============================] - 742s 380ms/step - loss: 2.9520 - acc: 0.3893\n",
            "\n",
            "Epoch 00038: acc improved from 0.38893 to 0.38926, saving model to drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5\n",
            "Epoch 39/100\n",
            "1953/1953 [==============================] - 742s 380ms/step - loss: 2.9397 - acc: 0.3915\n",
            "\n",
            "Epoch 00039: acc improved from 0.38926 to 0.39151, saving model to drive/My Drive/Machine_Learning-prj/weights-training-improvement-languagemodel.hdf5\n",
            "Epoch 40/100\n",
            "1953/1953 [==============================] - 742s 380ms/step - loss: 3.0480 - acc: 0.3752\n",
            "\n",
            "Epoch 00040: acc did not improve from 0.39151\n",
            "Epoch 41/100\n",
            " 912/1953 [=============>................] - ETA: 6:35 - loss: 3.2301 - acc: 0.3471"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-a0c88b5bef25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences_digit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences_digit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1313\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1315\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2228\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2229\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2230\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "JxN3rIb_dL26",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save('drive/My Drive/Machine_Learning-prj/vietnamese_language_model/39_weight_language_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cvSQ0uYmSD9f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}